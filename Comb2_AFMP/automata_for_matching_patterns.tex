%-*- coding:UTF-8 -*-
% 字符串匹配.tex
\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings} %插入代码
\usepackage{xcolor} %代码高亮
\usepackage{blkarray}
\usepackage{diagbox}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{color}
\usepackage{multirow}
\usepackage{color}
\usepackage[all,pdf]{xy}
\usepackage{verbatim}   %comment
\usepackage{cases}

% THEOREM Environments --------------------------------------------------------
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{prob}[thm]{Problem}
\newtheorem{mthm}[thm]{Main Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\numberwithin{equation}{subsection}
% MATH ------------------------------------------------------------------------
\DeclareMathOperator{\RE}{Re}
\DeclareMathOperator{\IM}{Im}
\DeclareMathOperator{\ess}{ess}
\newcommand{\eps}{\varepsilon}
\newcommand{\To}{\longrightarrow}
\newcommand{\h}{\mathcal{H}}
\newcommand{\s}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\BOP}{\mathbf{B}}
\newcommand{\BH}{\mathbf{B}(\mathcal{H})}
\newcommand{\KH}{\mathcal{K}(\mathcal{H})}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Complex}{\mathbb{C}}
\newcommand{\Field}{\mathbb{F}}
\newcommand{\RPlus}{\Real^{+}}
\newcommand{\Polar}{\mathcal{P}_{\s}}
\newcommand{\Poly}{\mathcal{P}(E)}
\newcommand{\EssD}{\mathcal{D}}
\newcommand{\Lom}{\mathcal{L}}
\newcommand{\States}{\mathcal{T}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\left<#1\right>}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\essnorm}[1]{\norm{#1}_{\ess}}


% Some setup
\pagestyle{plain}
\geometry{a4paper, top=2cm, bottom=2cm, left=2cm, right=2cm}
\CTEXsetup[format={\raggedright\bfseries\Large}]{section}
\lstset{numbers=left, %设置行号位置
        numberstyle=\small, %设置行号大小
        keywordstyle=\color{blue}, %设置关键字颜色
        commentstyle=\color{purple}, %设置注释颜色
        %frame=single, %设置边框格式
        escapeinside=``, %逃逸字符(1左面的键)，用于显示中文
        breaklines, %自动折行
        extendedchars=false, %解决代码跨页时，章节标题，页眉等汉字不显示的问题
        %xleftmargin=2em,xrightmargin=2em, aboveskip=1em, %设置边距
        tabsize=4, %设置tab空格数
        showspaces=false %不显示空格
       }

% About math
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand\Emph{\textbf}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcommand{\resetcounter}{\setcounter{equation}{0}}

\begin{document}

\title{\Huge 基于自动机原理的字符串匹配}
\vspace{2cm}
\author{\Large Trasier}
\date{\today}
\maketitle


\section{前言}

	字符串匹配的相关算法旨在解决在一个较大规模的文本中查询模式串的匹配情况。
	这里的模式串可能是一系列单词，也可能是一个强大的正则表达式。有许许多多
	的算法可以解决字符串匹配的相关问题，而这些算法往往包括\Emph{预处理}以及\Emph{搜索}
	两个阶段。其中一部分算法在搜索阶段往往采用\Emph{自动机}这一载体，自动机的优势
	主要在于通过状态的跃迁加快查找的速度，同时支持在线查找，可以处理大文件。
	
	文中涉及的算法主要使用\Emph{最小化的确定性有穷自动机}，用来处理\Emph{re-matching}、
    \Emph{dictinoary-matching}以及\Emph{index-matching}等问题。

\section{符号说明}
	
	简单规定一下使用的符号说明，后面的自动机算法都尽量以形式化的方法说明。
	
\subsection{Alphabet and words}
	
	\noindent
    $A$表示一个字符集合，$a$表示$A$的一个元素。				\\
	$u$表示一个单词，$|u|$表示长度，$u_j$表示第$j$个字符。	\\
	$u \cdot v$或$uv$表示单词$u$连接单词$v$，$u^0 = \varepsilon$，$uw^-1$表示单词$v, u=wv$。\\
	当$\exists u^0, u^1, u=u^0vu^1$成立时，称$v$是$u$的factor。
	此时若$v \neq u$并且$u^0=\varepsilon or u^1=\varepsilon$，我们称$v$是$u$的proper factor。
	
\subsection{Languages}
	
	\noindent
	$U$表示一个语言集合，$U \subseteq A^*$.	\\
	$pref(U), fact(U), suff(U)$分别表示语言$U$的某个前缀、子串、后缀。	\\
	$Pref(U), Fact(U), Suff(U)$分别表示语言$U$的所有前缀、子串、后缀的集合。	\\
	$|U| = \sum_{u \in U} |u|$。
	
\subsection{Regular expressions}

	就是平时用的RE。
	
\subsection{Finite automata}
	
	通常FA的定义是一个五元组，因为符号集合其实就是$A$。因此，这里简化一下，使用四元组来定义一个FA。
	$Q$表示状态集合，$i$表示初始状态，$T, T \subseteq Q$表示终结态，$E$表示状态转换函数。
	如果把一个FA理解成一个图的话，$Q, i, T$其实都是点，而$E$则表示边的集合。
	同时，$i$的入度为0。
	边往往采用三元组描述$(p, a, q)$，$p,q \in Q$，表示当前字符为$a$时，可由状态$p$转换为状态$q$。
	也可以表示为$\delta(p, a)=q$，从而可以得到如下递推式
	\[
		\delta(p, u) =
		\left\{
		\begin{aligned}
			&p,		  &if\ u=\varepsilon,	\\
			&\delta(\delta(p, a), v), &if\ u=av,	\\
			&undefined,  &otherwise.
		\end{aligned}
		\right .
	\]
	其实这个递推式就是描述了一个在自动机上搜索$u$的过程。
	
\section{DFA的抽象数据结构}

	很多模式匹配的算法都依赖于DFA的具体实现的数据结构，重点是能够实现DFA的转换函数。
	下面列举了主要的五种方法：
	transition matrix, adjacency lists, transition list, failure function, table-compression。
	
	我们将不同方法查询$\delta(p,a)$的时间花费称为$delay$, $delay$表示在查询下一个字符前花费在查询$a$的时间。
	一般来说，允许状态分支的数据结构的$delay$为常数，如transition matrix；而基于比较的数据结构的查询时间为
	$O(log |A|)$。
	
\subsection{Transition matrix}

	转移矩阵式最简单的DFA的实现方式，这也不难想象。
	还是讲DFA想象成一个directed graph，这其实就是个邻接矩阵。
	显然它的空间复杂度为$O(|Q| \times |A|)$，延迟为$O(1)$。
	它的缺陷和优势与邻接矩阵几乎相同。
	
\subsection{Adjancy lists}
	
	这个邻接表其实也和普通Graph里的邻接表是一致的。
	因此，它的空间复杂度为$O(|Q| + |E|)$，延迟为$O(log \min(|Q|, |A|))$。
	其实这里一般携程$O(log |A|)$就好了，当$|Q|<|A|$时，导致每个结点的出度至多为$|Q|$。
	因此，综合起来就是$O(log \min(|Q|, |A|))$。
	
\subsection{Transition list}
	
	这东西实际上就是个hash。（当发现没有比hash好很多的数据结构时，很多工程和算法都会用hash代替复杂数据结构）
	显然，可以把$e \in E$看成一个三元组$(p, a, q)$。
	这个hash的主要任务就是快速地映射$(p,a)$。
	空间至少是$O(|E|)$，时间平均为常量。
	
\subsection{Failure function}
	
	\Emph{Failure function}挺重要的，也挺有效的，后面的几种自动机模型都一定程度上使用了failure function。
	其实FA包括DFA, NFA, $\varepsilon$-NFA，对于NFA而言，可能存在$\delta(p,a)=q_0, \delta(p,a)=q_1$。
	然而DFA，对于给定的输入字符$a$以及当前状态$p$，仅存在唯一的$q, \delta(p,a)=q$。
	它的空间复杂度和时间复杂度都是线性的。	\\
	
	failure function是AC自动机算法中最不好理解的部分，一直没有弄懂fail都低是干嘛的，其实就是个线索树。
	我是这么理解fail函数的：
	使用确定化将NFA转化为DFA时，DFA的结点可能代表了NFA的几个状态集合，因为$\delta$函数指向的新的状态可能
	也是原始状态的集合。若还使用上述的方法就会造成空间爆炸。因此，这里使用fail函数将相关的状态关联起来，
	不断的递归进行检索，可以一定程度上降低空间复杂度。
	
	\begin{defn}
		$\gamma$表示函数$Q \times A \rightarrow Q$, $f$表示函数$Q \rightarrow Q$。
		通过$\gamma f$可重写$\delta$的递推式。
		\[
			\delta(p,a) =
			\left\{
			\begin{aligned}
				&\gamma(p, a), 		&if\ \gamma(p, a) \neq Null,	              \\
				&\delta(f(p), a), 	&if\ \gamma(p, a) = \text{Null and } f(p) \neq Null,	\\
				&i,					&otherwise.
			\end{aligned}
			\right .
		\]
	\end{defn}
	假设使用adjancy lists表示$\gamma$，则空间复杂度为$O(|Q| + |E'|)$。
	这里$E' = \{(p,a,q) | (p,a,q) \in E \ and\ \gamma(p,a) is defined \}$.
	时间复杂度为$O(|Q| \times log |A|)$。
	
\subsection{Table-compression}

	这个模型其实没什么，就是在failure function的基础上使用了邻接矩阵。
	$fail, |fail|=|Q|$还是原来的failure函数，$base, |base|=|Q|$表示状态$p$对应的出边对应的其实地址，
	$check, |check|=|Q|\times|A|$表示对应结点的状态，$target, |target|=|Q|\times|A|$表示下一状态。
	重写递推式
	\[
		\delta(p, a) =
		\left\{
		\begin{aligned}
			&target[base[p]+a], 	&if\ check[base[p]+a] = p		\\
			&\delta(fail(p), a),	&if\ check[base[p]+a] \neq p
		\end{aligned}
		\right .
	\]
	显然空间复杂度为$|Q| \times |A|$，时间复杂度降低为$O(|Q|)$。
	
	个人感觉，并没好到哪里去。
	
\section{Matching regular expressions}

\subsection{前言}

    \begin{prob}
        (正则表达式匹配) 给定一个正则表达式x, 找到x表示的所有字符串在目标串y上的位置。
    \end{prob}
	经典的算法是构建表达式$x$的$NFA$，将$y$作为自动机上的搜索串，
	查找所有可能的$y$的前缀，满足$Pref(y) \in A^*Lang(x)$。
	构建这个自动机的时间和空间均是$O(|x|)$，匹配$y$的时间是$O(|x||y|)$，
	识别$y$的每个字符的时间是$O(|x|)$。
	
\subsection{RE自动机}

	\begin{thm} $x$表示一个正则表达式，一定存在一个可以识别$x$并满足如下条件的$\varepsilon-NFA$：
		\begin{enumerate}[(1)]
		
			\item 状态总数上界为$2|x|$；
			
			\item 标号为$a$的边的上界为$|x|$，标号为$\varepsilon$的边的上界为$4|x|$；
			
			\item 每个状态的入度和出度的总和不超过2，并且当且仅当标号为$\varepsilon$时总和为2。
			
		\end{enumerate}
	\end{thm}
	
	这个定理还是比较重要的，这个为RE自动机算法的空间复杂度$O(|x|)$和时间复杂度$O(|x||y|)$奠定了基调。
	这个定理也比较好证明，基本思路是对两个表达式$x,y$进行数学归纳。证明$x+y, x \cdot y, x^*$也满足上述定理。
	
	\begin{thm}
		$\delta(x)$表示由正则表达式$x$构建的$\varepsilon-NFA$，空间和时间复杂度均满足$O(|x|)$。
	\end{thm}
	
\subsection{使用RE自动机搜索}

	这里面有个稍微难理解的东西$\varepsilon 闭包$。因为$\varepsilon-NFA$中的状态转移可能由$\varepsilon$引发，
	因此，对于$\delta(p,\varepsilon) = q$，当搜素到$p$时，同时也要把$q$加入带搜索集合中。
	因为，这个状态发生的可能。（DFA可以这个东西，但是会产生状态爆炸的问题。）
	
	使用RE自动机搜素主要包括\Emph{Closure}和\Emph{Transitions}两个过程。
	\begin{lstlisting}[frame=shadowbox,framexleftmargin=5mm,rulesepcolor=\color{gray},numbers=none]
		Closure(E, S)
			R \leftarrow S
			\vartheta \leftarrow EmptyQueue
			for each state p in S
				loop Enqueue(\vartheta, p)
			while not QueueIsEmpty(\vartheta)
				loop p \leftarrow Dequeue(\vartheta)
					for each state q such that(p, \varepsilon, q) is in E
						loop if q is not in R
							then R \leftarrow R + {q}
								Enqueue(\vartheta, q)
			return R
	\end{lstlisting}
	\begin{lstlisting}[frame=shadowbox,framexleftmargin=5mm,rulesepcolor=\color{gray},numbers=none]
		Transitions(E, S, a)
			R \leftarrow \emptyset
			for each state p in S
				loop for each state q such that (p, a, q) is in E
					loop R \leftarrow R + {q}
			return R
	\end{lstlisting}
	
	这样一个自动机的基本功能是用来检测字符串$y$是否可以被正则表达式$x$所表示，这样一个过程称为\Emph{Tester}。
	\begin{lstlisting}[frame=shadowbox,framexleftmargin=5mm,rulesepcolor=\color{gray},numbers=none]
		Tester(x, y)
			Build RE (Q, i, {t}, E) of x
			C \leftarrow Closure(E, {i})
			for letter a from first to last letter of y
				loop C \leftarrow Closure(E, Transition(E, C, a))
			return t \in C
	\end{lstlisting}
	
	\begin{prop}
		给定一个正则表达还是$x$，测试单词$y$属于$Lang(x)$的时间复杂度为$O(|x||y|)$，空间复杂度为$O(|x|)$。
	\end{prop}
	
	\begin{proof}
		其实只能说Transitions和Closure可以在$O(|Q|)=O(|x|)$中实现，这样Tester的复杂度就是$O(|y||x|)$。
	\end{proof}
	
	Tester只能做判定，还是回到最初的问题，识别所有可以匹配$x$的$fact(y)$，将这样的$fact(y)$在y中的右边界的位置称为$end-point$。
	做法很简单，在每次Tester的循环中都将初始状态$i$加入$C$中，表示每次搜索可能都是从初始状态开始。
	这样就得到了过程Matcher。
	
	\begin{lstlisting}[frame=shadowbox,framexleftmargin=5mm,rulesepcolor=\color{gray},numbers=none]
		Matcher(x, y)
			Build RE (Q, i, {t}, E) of x
			C \leftarrow Closure(E, {i})
			occurrence if t \in C
			for letter a from first to laster letter of y
				loop C \leftarrow Closure(E, Transition(E,c,a))
					occurrence if t \in C
	\end{lstlisting}
	
\subsection{时间和空间的trade-off}
	
	上述方法是使用了$\varepsilon-NFA$这种自动机模型，其实也可以使用$DFA$。
	但是在对状态进行确定化时，往往会产生空间爆炸。
	如$x = a(a+b)^{m-1}$，识别$A^*Long(x)$的$DFA$需要$2^m$个状态。
	因为，它需要记录$m$个后缀字符。在构建$DFA$时往往采用延迟构建技巧。
	
	% Lazy Construction这东西在vEB里面就见到了，在那里面还比较好理解
	% 但在DFA的建立里面就理解乏力了？莫非等价于failure function。

\section{Matching Dictonary}

\subsection{前言}
	
	\begin{prob}
		（字典匹配问题）给定有限的单词集合$X$，找到$\forall x \in X$在字符串$y$中匹配的所有位置。
	\end{prob}
	
	解决这个问题的经典算法是Aho and Corasick（AC自动机）。
	它的基本思路思路是实现一个基于线性空间复杂度的$DFA$识别语言$A^*X$。
	这个$DFA$的实现往往会结合邻接表和failure function。
	
	\begin{mthm}
		\Emph{(Aho and Corasick 1975).} 面向单词集合$X$和字符串$y$的字典匹配问题，可以在如下条件内实现：
		\begin{itemize}
			\item 在预处理阶段，构建基于$X$的可识别语言$A^*X$的$DFA$，时间复杂度为$O(|X|\times|A|)$，空间复杂度为$O(|X|)$
			\item 在搜索阶段，使用$y$在$DFA$上进行搜索，时间复杂度为$O(|y|\times|A|)$，空间复杂度为常量，$delay$的时间花销为$O(|X|\times|A|)$
		\end{itemize}
	\end{mthm}
	% delay很有可能比整体的时间复杂度大，怎么破？
	
	首先思考问什么一直强调这个自动机是能够识别语言$A^*X$的。
	\begin{proof}
		若$\forall x \in X$并且$x$出现在$y$中，即$x \in fact(y) \Rightarrow y=uxv \Rightarrow y=A^*xv$。
		这也意味着，使用$y$在自动机$DFA$中进行搜索时，遇到的任何对应终$A^*x$终结态的位置即为所求。
	\end{proof}
	
\subsection{Dictionary-matching automata}
	
	为了形式化的定义识别语言$A^*X$的$DFA$，引入基于语言$U$、字符串$v$的映射$h_U: A^* \rightarrow Pref(U) $, 并且
	\[
		h_U(v) = v', v' \in Suff(v) \cap Pref(U), |v'| \ge \max_{vv \in Suff(v)\cap Pref(U)} {|vv|}
	\]
	从而，可以得到字典匹配问题的$DFA$的形式化定义
	\begin{defn}
		若$X$表示一个有限语言集合，那么可以识别语言$A^*X$的确定性有限自动机可以表示为
		\[
			\Big( Pref(X), \varepsilon, Pref(X) \cap A^*X, \{(p,a,h_X(pa) | p \in Pref(X), a \in A\} \Big)
		\]
	\end{defn}
	这个定义很重要也很有意思，想要理解AC自动机到底在干嘛主要需要理解好这个定义。
	$(Q, i, T, E)$四元组表示一个$FA$。
	首先，最好理解的是初始状态为$\varepsilon$表示一个空串，这是显然的。
	
	其次，状态集合为$Pref(X)$，表示$X$的所有前缀。这里$X$并不是一个正则表达式，
	因此也可以比表示为$X = \bigcap_{x \in X} Pref(x)$，（这也就意味着其实AC自动机上的每个结点其实表示的是一个前缀而非一个字符，
	但是只存一个字符显然可以省空间，因此仅存一个字符，从trie的根一直搜索到当前结点的路径就是这个前缀，这是后话，留个印象）。
	那为什么要用前缀呢？
	关键是要识别$A^*X$。不妨想象$X={x}$，因此，问题转化成两个字符串的匹配。
	可以直接使用$O(|x||y|)$的朴素算法在字符串$y$的每个位置，搜索是否可以匹配一个$x$，
	其实就是一个字符一个字符的比较是否和$x$中对应的字符相等，
	因此，$\forall i,j < |x|, i<j$当且仅当搜索到了$i$才可以搜索到后续位置$j$。
	不难发现，$i,j$其实代表的都是$x$的位置。
	那么，对于$|X|>1$，其实我们也要找到能匹配的所有前缀，才会最终判定是否匹配完整的$x, x \in X$。
	
	从而我们可以理解终结状态是一个集合，即$Pref(X) \cap A^*X$，
	其实就是$\forall x \in X$对应的状态。（可以理解成匹配字典中一个完整的单词）
	
	最后，是转换函数${(p,a,h_X(pa) | p \in Pref(X), a \in A}$，这里使用了$h_U$的定义，不妨令$\delta(p,a)=q$
	$q \in Pref(X)$这个最好理解，因为$q \in Q$这是必然的，因此$q \in Pref(X)$。
	$q \in Suff(pa)$这个也还好理解，毕竟是要匹配$A^*X$，可以把任何$pa$的前缀字符归为$A^*$，我们只需要后缀。
	关键是为什么我们需要这个新的前缀$q, q \in Pref(X)$最长。
	（这也是为什么Dictionary-matching时，failure function是靠谱的，这也是为什么很多人都说AC自动机=trie+kmp，确实可以从kmp的next数组这儿理解）。
	
	\begin{lem}
		不妨令$U \subseteq A$，那么有
		\begin{enumerate}[(1)]
			\item $v \in A^*U \text{iff } h_U(v) \in A^*U, \text{for each } v \in A^*$;
			\item $h_U(\varepsilon) = \varepsilon$;
			\item $h_U(va) = h_U(h_U(v)a), \text{ for each } (v, a) \in A^* \times A$.
		\end{enumerate}
	\end{lem}
	\begin{proof}
		前两个性质是显然的，关键是第三个。这也是为什么$Dictionary-matching problem$可以配合使用failure function的原因。
		显然$h_U(va) \in Suff(va), h_U(v)a \in Suff(va)$，进而其中之一必是另一个的后缀。因此，可以分两种情况讨论：
		\begin{enumerate}[(1)]
			\item $h_U(v)a$是$h_U(va), h_U(va) \neq \varepsilon$的后缀。
			
			不妨令$wa = h_U(va), w \in Pref(U)$，因此，$h_U(v)$是$w$的后缀，
			而由$|h_U(v)| > |w|$，因此，$w$又是$h_U(v)$的后缀，两者矛盾，显然这种情况不成立。
			% 其实这从另一个角度证明了$|h_U(v)a| \ge |h_U(va)|$的。
			
			\item $h_U(va)$是$h_U(v)a$的后缀。
			
			$h_U(va) \in Pref(U), h_U \in Suff(h_U(v)a)$，应用$h_U$的定义可知。
			$|h_U(va)| \le |h_U(h_U(v)a)|$，因此$h_U(va)$必定是$h_U(h_U(v)a)$的后缀。
			而$h_U(h_U(v)a) \in Pref(U), h_U(h_U(v)a) \in Suff(va)$，再次应用$h_U$的定义可知。
			$|h_U(va)| \ge |h_U(h_U(v)a)|$，因此，当且仅当$h_U(va) = h_U(h_U(v)a)$成立时，才满足条件。
			% 条件三是不是和kmp的next数组的使用很像，这个定理还是挺重要的。
			
		\end{enumerate}
	\end{proof}

	这个定理$(3)$还是挺重要的，这也从侧面证明了为什么$\delta(p,a)=h_X(pa)$。
	\begin{proof}
		$p = h_X(v_1v_2 \cdots v_p), \delta(p, a) = h_X(h_X(v_1v_2 \cdots v_p)a) = h_X(v_1v_2 \cdot v_p a)=h_X(pa)$
	\end{proof}

\end{document}
