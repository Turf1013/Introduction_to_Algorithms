\section{Experimental Study}
\label{sec:experiment}

\subsection{Experiment Setup}

\textbf{Dataset.}
We use the Foursquare dataset from \cite{yang2014modeling} as our real dataset and 
this dataset contains the check-in activities of users from Foursquare\footnote{https://foursquare.com/} in two cities, i.e., New York and Tokyo.
In the Foursquare dataset, each user is viewed as a worker, who has a check-in venue with his current location. 
Since there is no information about the exact location of each venue, 
we use the average value of the locations where users have cheked in as the location of the venue.
Each venue with more than 20 check-in is viewed as the location of a task.
We randomly generate from 1 to 5 tasks at each venue.
We also generate the historical accuries of workers following normal distribution and set the capacity of a worker as 4,
because all these information is not given in the data set.
The statistics of real data are illustrated in Table~\ref{table:dataset-real}.
We also use a synthetic dataset for evaluation. 
We randomly generate the locations of workers and tasks in a $1000 \times 1000$ 2D grid.
The value of $d_{max}$ is set to be 30. 
The statistics and configuration of synthetic data are illustrated in Table~\ref{table:dataset-syn}, where we mark our default settings in bold font.

We evaluate the Random, MCF-LTC, Large Acc First and Average And Max algorithms in terms of 
the maximum index of worker, running time and memory cost. 
Random is used to be the baseline algorhtm. 
When a new worker appears, Random selects tasks from nearby randomly under the capacity of the worker.
We study he effect of varying parameters on the peformance of the algorithms.
In each experiments, we repeat 30 times and report average results.
The algorithms are implemented in GNU C++, and the experiments were conducted on a server with 32 Intel
Xeon E5 2.4GHz processors with Hyper-Threading enabled and 16GB memory.

\begin{table}
	\centering
	\caption{Real Dataset}
	\vspace{-1ex}
	\label{table:dataset-real}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Dataset & $|T|$ & $|W|$ & K & $\epsilon$ & Accuracy \\
		\hline
		New York & 7153 & 395564 & \multirow{2}{*}{4} & \multirow{2}{*}{[0.12, 0.14, 0.16, 0.18, 0.2]} &$\mu = 0.85$ \\
		\cline{1-3}
		Tokyo	& 15405 & 573703 & & &$\sigma = 0.05$ \\
		\hline
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Synthetic Dataset}
	\vspace{-1ex}
	\label{table:dataset-syn}
	\begin{tabular}{|c|c|}
		\hline
		Factor & Setting\\
		\hline
		$|T|$ & 500, 1000, \textbf{2500}, 5000, 10000\\
		\hline
		$|W|$ & 	\textbf{40000}		\\
		\hline
		$K$ & 2, 4, \textbf{6}, 8, 10\\
		\hline
		$\epsilon$ & 0.01, 0.05, \textbf{0.1}, 0.15, 0.2\\
		\hline
		{Historical} & \textbf{Normal}: $\mu = [0.75,  0.80, \textbf{0.85}, 0.90, 0.95], \sigma=0.05$	\\
		{Accuracy} &Uniform: $[0.5, 1.0], [0.6, 1.0], [0.7, 1.0], [0.8, 1.0], [0.9, 1.0]$ \\
		\hline
		\multirow{2}{*}{Scalability} & $|T|$ = 10K, 20K, 30K, 40K, 50K, 100K \\ & $|W|$ = 400K\\
		\hline
	\end{tabular}
\end{table}

\subsection{Experiment Results}

\begin{figure*}[htb]
	\centering
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/1.pdf}
		\caption{\footnotesize{Latency of varying $|T|$}}
		\label{fig:card_t_sum}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/4.pdf}
		\caption{\footnotesize{Latency of varying $K$}}
		\label{fig:card_w_sum}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/10.pdf}
		\caption{\footnotesize{Latency of varying accuracy(Normal)}}
		\label{fig:card_p_sum}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/13.pdf}
		\caption{\footnotesize{Latency of varying accuracy(Uniform)}}
		\label{fig:cf_sum}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/2.pdf}
		\caption{\footnotesize{Runtime of varying $|T|$}}
		\label{fig:card_t_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/5.pdf}
		\caption{\footnotesize{Runtime of varying $K$}}
		\label{fig:card_w_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/11.pdf}
		\caption{\footnotesize{Runtime of varying accuracy(Normal)}}
		\label{fig:card_p_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/14.pdf}
		\caption{\footnotesize{Runtime of varying accuracy(Uniform)}}
		\label{fig:rw_time}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/3.pdf}
		\caption{\footnotesize{Memory of varying $|T|$}}
		\label{fig:card_t_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/6.pdf}
		\caption{\footnotesize{Memory of varying $K$}}
		\label{fig:card_w_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/12.pdf}
		\caption{\footnotesize{Memory of varying accuracy(Normal)}}
		\label{fig:card_p_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/15.pdf}
		\caption{\footnotesize{Memory of varying accuracy(Uniform)}}
		\label{fig:cf_mem}
	\end{subfigure}
	\caption{Results on varying cardinality, capacity, and historical accuracy}
	\vspace{-3ex}
	\label{fig:exp1}
\end{figure*}

\textbf{Effect of cardinality of $T$}.
The result of varying cardinality of $T$ is shown in the first column of Figure~\ref{fig:exp1}. 
In terms of the maximum index of workers, first we can observe that the result of MCF-LTC becomes worse with the increase of the $T$. 
The reason is that is when $T$ is large, the size of each batch is large. 
Then some last few workers in a batch may perform better than the others. 
Therefore MCF-LTC algorithm may choose them even though their index is large. 
This may make the result become worse when the size of a batch is large. 
Second we can observe that the result of Average And Max becomes notably better than the others when $T$ becomes large. 
Finally, in terms of the running time and memory, MCF-LTC algorithm consumes more than others. 
Considering all these three terms, Average And Max performs the best when the cardinality of $T$ varies.

\textbf{Effect of capacity $K$}.
We next study the effect of capacity of workers. 
The second column of Figure~\ref{fig:exp1} illustrates the results when the capacity is varied from 2 to 10. 
In terms of the maximum index of worker, first we can observe that the results of all algorithms drops down with the increase of the capacity. 
This change is reasonable since the result can be reduced when workers can perform more tasks. 
Second, the results drop really fast at the beginning when the capacity increases from 2 to 4, but it becomes slowly when the capacity increases from 4 to 10. 
This indicates that result is more easily reduced when the capacity is small. 
Finally, the result of Average And Max is always better than the other two online algorithms, 
Random and Large Acc First. 
The result of offline algorithm MCF-LTC becomes the best when the capacity becomes large. 
owever, it performs worse than Average And Max when the capacity is small. 
That is because when capacity is small, the size of a batch is large and with the same aforementioned reason the result of MCF-LTC may easily become worse due to the large size.
 In terms of running time and memory, MCF-LTC still consumes the most and the other three algorithms consume nearly the same amount.

 \textbf{Effect of distribution of historical accuracy}.
We next study the effect of historical accuracy under different distributions.
.The result is shown in the third column of Figure~\ref{fig:exp1} when historical accuracy is under normal distribution with a varying $\mu$.
In terms of the maximum index of worker, MCF-LTC always performs the best, but the result of Largest Acc First and Average And Max algorithm become closed to MCF-LTC from $\mu = 0.85$ to $\mu = 0.1$. 
Since the tolerate error rate $\epsilon$ is 0.1, it is reasonable that greedy algorithms may perform a better result when $\mu$ becomes near to $1.0 - \epsilon$.
The last column of Figure~\ref{fig:exp1} shows the result when historical accuracy is under uniform distribution with varying mean.
In terms of the maximum index of worker, MCF-LTC performs the best. 
A possible reason is that when the accuracy is uniformaly distributed, it becomes easier for greedy to fall into the local optimal.
Even when mean is $0.95$, every worker is highly accurate since the tolerate error rate is 0.1 .
But still MCF-LTC performs better than the others.
For running time and memory in both distributions, MCF-LTC still consumes the most and all other algorithms are very closed.

\begin{figure*}[htb]
	\centering
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/7.pdf}
		\caption{\footnotesize{Latency of varying error rate}}
		\label{fig:muP}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/16.pdf}
		\caption{\footnotesize{Latency of Scalabiltiy}}
		\label{fig:lambdaP}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/19.pdf}
		\caption{\footnotesize{Latency of New York}}
		\label{fig:muU}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/22.pdf}
		\caption{\footnotesize{Latency of Tokyo}}
		\label{fig:lambdaU}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/8.pdf}
		\caption{\footnotesize{Runtime of varying error rate}}
		\label{fig:card_v_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/17.pdf}
		\caption{\footnotesize{Runtime of Scalability}}
		\label{fig:card_u_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/20.pdf}
		\caption{\footnotesize{Runtime of New York}}
		\label{fig:dim_time}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/23.pdf}
		\caption{\footnotesize{Runtime of Tokyo}}
		\label{fig:cf_time}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/9.pdf}
		\caption{\footnotesize{Memory of varying error rate}}
		\label{fig:card_v_mem}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/18.pdf}
		\caption{\footnotesize{Memory of Scalability}}
		\label{fig:card_u_mem}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/21.pdf}
		\caption{\footnotesize{Memory of New York}}
		\label{fig:dim_mem}
	\end{subfigure}
	~~
	\begin{subfigure}[b]{0.22\textwidth}
		\includegraphics[width=\textwidth]{figure/24.pdf}
		\caption{\footnotesize{Memory of Tokyo}}
		\label{fig:cf_mem}
	\end{subfigure}
	\caption{Results on error rate, scalability and real data set}
	\vspace{-3ex}
	\label{fig:exp2}
\end{figure*}

\textbf{Effect of tolerate error rate $\epsilon$}.
We next study the effect of tolerate error rate $\epsilon$. 
Thus first column of Figure~\ref{fig:exp2} illustrates the results when we vary the tolerate error rate from $0.01$ to $0.2$. 
First, MCF-LTC performs better when the requirement of error rate is very strict.
But when the error rate is between 0.1 and 0.2, Average And Max performs a little better. 
Since according to the normal distribution of accuracy $\mu=0.85,\sigma=0.05$, 
it needs less worker to satisfy the requirement of error rate.
But the size of a batch in MCF-LTC is around $1.5$ thousands, which is a big portion compared with the results between $\epsilon = 0.1$ and 0.2.
Therefore, the peformance of MCF-LTC may become worse due to the large size of batch.
In terms of running time and memory, we can observe similar patterns as those of the aforementioned experiment results.

\textbf{Effect of Scalability}.
The MCF-LTC algorithm is not efficient enough according to our previous experiment results.
Thus we study the other algorithms in this part.
The result is shown in the second column of Figure~\ref{fig:exp2}.
Specifically, we vary the number of tasks from 10 thousand to 100 thousand.
In terms of max index of workers, Largest Acc First algorithm and Average And Max algorithm are very closed
at the beginning, which is reasonable according to their competitive ratio.
But when the scalability becomes large, Average And Max performs better than the other two algorithms.
Even though it takes the longest time and needs the most memory, but the gap in running time and memory is relatively small.

\textbf{Real dataset}.
Finally, the results on real datasets are shown in the last two columns of Figure~\ref{fig:exp2}.
Since the MCF-LTC algorithm is not efficient enough, we study the performance of other algorithms with varying the tolerate error rate $\epsilon$.
In terms of the maximum index of worker, Average And Max algorithm performs the best on dataset of both New York and Tokyo.
In terms of running time and memory, Average And Max is only a bit more than others 
with a few seconds longer and no more than 1MB memory.

\subsection{Experiment Summary}
In terms of the maximum index of workers, 
Average And Max algorithm performs the best in many of the experiments, especially when the scalability is large.
In most experiments, Average And Max algorithm is better than Random and Large Acc First.
But sometimes MCF-LTC algorithm performs the best especially when the capacity of worker is large, the tolerate error rate is small and the distribution of accuracy is uniform.
In the aspect of running time and memory cost, MCF-LTC algorithm always performs the worst while Large Acc First usually peforms the best.
Overall, Average And Max is not only effective but also scalable in both datasets.
